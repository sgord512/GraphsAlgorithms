Title: Friday Night's Alright for Fighting, pt. 2
Status: Publish
DateGMT: 2011-04-11 08:20:41
PostName: friday-nights-alright-for-fighting-pt-2
---
So yeah, here is my newest post. In which I continue <a href="http://starlingsoftheslipstream.wordpress.com/2011/04/09/friday-nights-alright-for-fighting/">this post</a>. So read that, or risk not knowing some important context. Nah, just kidding.

Here's where we were: I was going to formulate a mission statement for my blog. 

Here's one: I will write whatever in the world I care about. I will have no mission. The only thing I will try to do is keep my posting regular.

So I was thinking a while back about how programming is just way harder than it should be, and I had been unable to articulate exactly why that was the case. Well, I was reading the excellent <i>Algorithmics</i>, Third Edition, by David Harel with Yishai Feldman, and they had a section on this very topic. I thought it was excellently done, and so I'll share with you the summary of what they said: So their first point was that as you increase the size of a computer program you exponentially increase the number of different states that you have to deal with. In continuous systems, you can slide along a number of quantitatively different states without having something that is qualitatively different. The example they use is driving a car: Despite there being an essentially infinite number of speeds that the car can drive at, there are up to five different positions for a manual transmission, and all speeds at a particular gear are the same for the driver. On the other hand, because all changes are discrete in a computer system, there is no way to capture something quantitatively different without having flipping some bits. Every bit you add to the system doubles the number of possible states the system can be in. 

For those who are mathematically inclined, this discreteness means that computer systems are utterly intractable to analysis (meaning calculus). There is literally no such thing as a smooth change in a computer, which, when you think about it, is fundamentally different from our everyday experience and makes for an entirely different way of thinking about things. Smooth systems also get the advantage of cushioning, in that engineers can build bridges that are designed to withstand three times as much weight as is expected to be crossing the bridge at any time. By sacrificing efficiency a little bit, they are able to dramatically increase safety, and protect themselves from unexpected conditions. But all unexpected conditions in a computer are fundamentally different, and so there is no way to build a program that is tolerant. It's hard to understand what that would even mean? Even the smallest changes are carried through the program, and so the design has to be airtight to protect against the effects of any errors. In the bridge example, on the other hand, if more weight than expected is on the bridge due to a traffic jam, so long as the tolerance is great enough the system can react accordingly, as the structure absorbs the excess weight.

Now add in the fact that software can be changed and distributed rapidly, in contrast to a bridge which is made of metal and concrete. To change the bridge, you have to break out sledgehammers at the very least, and for anything at all involved, you are bringing in cranes and cement trucks, and bombing things. To change software, you never need more than one guy at a keyboard. This superficial ease of modification makes people think that software should be highly responsive, and bug-free. Why shouldn't a developer be able to make fixes as soon as they are spotted, and why shouldn't it be simple to implement your suggestion. I mean, it's just glowing dots on a screen. It's not like it's set in stone. 

Well, it may not be set in stone, but any sufficiently large program is far too large to be understood completely, and is so complex that it is often very very difficult to know how a change in one place will affect another location. It is simply a fact that most systems powerful enough to be useful to people are so complex that they are impossible to analyze. To prove that they are correct is an even harder problem, and to make it even worse, it has been proven that this problem in general is undecidable. Let me repeat this, because it takes a while to sink in: There is no hope of finding a general solution to the problem of verifying software, which means you can never be guaranteed to be bug-free. 

That makes things hard enough: then add in the fact that human beings have a lot of trouble reading and understanding code, even code they themselves wrote, and you end up with a discipline that counter-intuitively judges code quality by how easy it is for human readers to understand, and encourages a way of coding that is all about human understanding at the expense of computational efficiency and program brevity. 

Current research in computer science is all about trying to make coding easier, and there are a number of approaches that have been tried. A lot of people who are unfamiliar with programming look at the proliferation of programming languages, and are baffled. To them it seems utterly redundant to have both Python and Ruby, Perl and Scheme and Lisp and Racket, and C and Haskell and Lua and Scala and Groovy and Ada and Go and OCaml and Tcl and Rust and Eiffel and Erlang and Euphoria. (I kinda went overboard there, but I really like the names that programming languages end up with. They are so strange, and evocative.) While there is an argument to be made that the increased competition in the modern market for programming languages is inspiring them to continually improve, the more fundamental reason for this chaotic jumble of languages is more subtle. 

Since Alan Turing's landmark paper sometime in the first couple decades of the 20th century, it has been known that all sufficiently powerful programming languages are essentially equivalent, in that they are capable of expressing the same programs. Any language that satisfies this requirement is called Turing-complete, and the vast majority of popular languages these days are Turing-complete. What makes each language different then is the way in which it forces someone programming in that language to think about the task at hand. When you start coding in a particular language, you become subtly influenced by that language's syntax and semantics, and only when you switch to another language are you able to see the way in which each language presents an entirely different worldview. For example, in Racket and Scheme, the fundamental unit of data is the list. When you think about performing operations, you think about recursively traversing lists, and rebuilding them in strange and creative ways. When you code in Java, you think of designing a set of components that you are going to then fit together, and you try to make sure each component is designed so that it interacts with the others only as much as it needs to. Literate programming, a relatively new paradigm based on human readability, aims for code that flows on the page, and where the documentation is integrated into the code itself, and is interactive.

And that's some thoughts on computer science. I'm done for today, but I'll be back soon.

From a far corner of the slipstream, sliding down waterfalls, and blue electric veins, through crisp black buzzing city nights, whistling in the dark,
So long, 
Spencer
---
